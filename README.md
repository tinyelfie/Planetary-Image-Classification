Planetary Image Classification Using Classical Machine Learning

This repository presents a complete classical machine learning pipeline for the classification of planetary images. The dataset consists of images representing eight planets, two dwarf planets (Pluto and Makemake), and the Moon. The project demonstrates the effectiveness of feature engineering techniques—specifically Local Binary Patterns (LBP), Histogram of Oriented Gradients (HOG), and Gray-Level Co-occurrence Matrix (GLCM) features—in enabling non-deep-learning models such as Support Vector Machines, Random Forests, and K-Nearest Neighbors to perform accurate classification of astronomical objects.

The project begins with the organization of the dataset into class-specific directories, followed by pre-processing of the images. Each image is resized to a uniform resolution, converted to grayscale, normalized, and flattened. These pre-processing steps ensure uniformity and consistency across the dataset, which is essential for reliable feature extraction and model performance.

Feature extraction forms the core of this project. Classical image descriptors are used to quantify texture, shape, and structural characteristics of celestial bodies. Local Binary Patterns capture fine-grained texture information; GLCM provides statistical insights into pixel relationships through contrast, homogeneity, correlation, and other metrics; and HOG captures edge orientation patterns that help distinguish characteristic structures such as planetary rings, cloud bands, or cratered surfaces. The combination of these features results in a rich, high-dimensional representation of each image, suitable for machine learning.

In parallel with supervised learning, unsupervised learning techniques are employed to study natural groupings within the dataset. Principal Component Analysis (PCA) is used to reduce the high-dimensional feature space into two dimensions for visualization while preserving maximum variance. The project also applies K-Means clustering to identify inherent groups in the data without using labels. The effectiveness of clustering is measured through silhouette scores, and PCA scatter plots are used to visually compare cluster assignments with ground truth labels. These analyses reveal meaningful geological and visual similarities among planets, such as the natural grouping of the ice giants Uranus and Neptune or the distinct separation of the Moon due to its highly textured surface.

Supervised classification is conducted using Support Vector Machines, Random Forests, and K-Nearest Neighbors. The dataset is divided into training and testing portions to ensure unbiased evaluation. For each model, the project reports standard classification metrics including accuracy, precision, recall, and F1-score. Confusion matrices are generated to provide a detailed breakdown of performance on individual classes. Support Vector Machines generally achieve the highest accuracy, reflecting their effectiveness in handling high-dimensional feature spaces created by HOG, LBP, and GLCM.

The project concludes with a comprehensive analysis that compares supervised and unsupervised results. The analysis highlights patterns in misclassification, explains which planets are most easily distinguishable, and shows how feature engineering contributes to improved model performance. It also discusses cases where planets with similar visual characteristics, such as Uranus and Neptune, exhibit overlapping feature representations that lead to occasional misclassification. The project demonstrates that classical machine learning techniques, when paired with strong feature engineering, can effectively classify astronomical images with high interpretability and transparency—qualities often less accessible in deep learning models.

This repository is organized into clearly separated sections containing code notebooks for each step of the workflow, stored models, visual outputs, and an optional project report. It is intended both as an academic demonstration of classical image classification techniques and as an industry-relevant example of interpretable machine learning for visual recognition tasks.

If further extensions are desired, such as model tuning, deployment through a lightweight web interface, or incorporation of color-based features, these can be added as future improvements.
